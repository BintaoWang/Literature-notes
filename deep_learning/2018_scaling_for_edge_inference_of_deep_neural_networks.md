# Note for [Scaling for edge inference of deep neural networks](https://cadlab.cs.ucla.edu/beta/cadlab/sites/default/files/publications/Shi_et_al_Perspective_1524052852_1.pdf)
## 摘要
&nbsp;&emsp;&ensp;Deep neural networks offer considerable potential across a range of applications, from advanced manufacturing to autonomous cars. A clear trend in deep neural networks is the exponential growth of network size and the associated increases in compu- tational complexity and memory consumption. However, the performance and energy efficiency of edge inference, in which the inference (the application of a trained network to new data) is performed locally on embedded platforms that have limited area and power budget, is bounded by technology scaling. Here we analyse recent data and show that there are increasing gaps between the computational complexity and energy efficiency required by data scientists and the hardware capacity made avail- able by hardware architects. We then discuss various architecture and algorithm innovations that could help to bridge the gaps.<br>



